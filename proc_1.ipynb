{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(y, y_pred):\n",
    "    \n",
    "    return roc_auc_score(y, y_pred)\n",
    "\n",
    "\n",
    "def gini_score(y, y_pred):\n",
    "    \n",
    "    GINI = (2 * roc_auc(y, y_pred)) - 1\n",
    "    \n",
    "    return GINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "binnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = ['uniform', 'kmeans', 'quantile']\n",
    "\n",
    "def k_binning(data, feature, n_bins, strategy = 'uniform'):\n",
    "    \n",
    "    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy = strategy)\n",
    "    \n",
    "    est.fit(data[feature].values)\n",
    "    \n",
    "    Xt = est.transform(data[feature].values)\n",
    "    \n",
    "    data[feature] = pd.DataFrame(Xt)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bins(df, bins_num, feature, label):\n",
    "    \n",
    "        df = df[[feature, label]]\n",
    "        \n",
    "        df['bin'] = pd.qcut(df[feature], bins_num, duplicates='drop') \\\n",
    "                    .apply(lambda x: x.left) \\\n",
    "                    .astype(float)\n",
    "        return df\n",
    "\n",
    "def generate_correct_bins(df_origin, feature, label, bins_max=20, bin_min_size = 5):\n",
    "    \n",
    "        for bins_num in range(bins_max, 1, -1):\n",
    "            \n",
    "            df = generate_bins(df_origin, bins_num, feature, label)\n",
    "            \n",
    "            df_grouped = pd.DataFrame(df.groupby('bin') \\\n",
    "                                      .agg({feature: 'count',\n",
    "                                            label: 'sum'})) \\\n",
    "                                      .reset_index()\n",
    "            \n",
    "            r, p = stats.stats.spearmanr(df_grouped['bin'], df_grouped[label])\n",
    "\n",
    "            if (\n",
    "                    abs(r)==1 and                                                        \n",
    "                    df_grouped[feature].min() > bin_min_size                   \n",
    "                    and not (df_grouped[feature] == df_grouped[label]).any()      \n",
    "            ):\n",
    "                break\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "def group_by_feature(df, feature, label):\n",
    "        df = df.groupby('bin') \\\n",
    "                            .agg({label: ['count', 'sum']}) \\\n",
    "                            .reset_index()\n",
    "        \n",
    "        df.columns = [feature, 'count', 'good']\n",
    "        df['bad'] = df['count'] - df['good']\n",
    "        return df\n",
    "    \n",
    "    \n",
    "def perc_share(df, group_name):\n",
    "    \n",
    "        return df[group_name] / df[group_name].sum()\n",
    "\n",
    "def calculate_perc_share(df, feature, label):\n",
    "    \n",
    "        df = group_by_feature(df, feature, label)\n",
    "        \n",
    "        df['perc_good'] = perc_share(df, 'good')\n",
    "        \n",
    "        df['perc_bad'] = perc_share(df, 'bad')\n",
    "        \n",
    "        df['perc_diff'] = df['perc_good'] - df['perc_bad']\n",
    "        \n",
    "        return df\n",
    "\n",
    "def woe_binning(df, feature, label):\n",
    "    \n",
    "        df = calculate_perc_share(df, feature, label)\n",
    "        \n",
    "        df['woe'] = np.log(df['perc_good']/df['perc_bad'])\n",
    "        \n",
    "        df['woe'] = df['woe'].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "\n",
    "df = pd.DataFrame({'col1' : data['data'][:,0], 'col2' : data['data'][:,1], 'label' : data['target']})\n",
    "\n",
    "df = df[df['label'].isin([0, 1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bined = generate_correct_bins(df, 'col1', 'label', 20, 4.1)\n",
    "df_res = woe_binning(df_bined, 'col1', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>count</th>\n",
       "      <th>good</th>\n",
       "      <th>bad</th>\n",
       "      <th>perc_good</th>\n",
       "      <th>perc_bad</th>\n",
       "      <th>perc_diff</th>\n",
       "      <th>woe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.299</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.72</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>-2.197225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.100</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.268264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.700</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.56</td>\n",
       "      <td>3.367296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    col1  count  good  bad  perc_good  perc_bad  perc_diff       woe\n",
       "0  4.299     40     4   36       0.08      0.72      -0.64 -2.197225\n",
       "1  5.100     30    17   13       0.34      0.26       0.08  0.268264\n",
       "2  5.700     30    29    1       0.58      0.02       0.56  3.367296"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iv_binning(df):\n",
    "    \n",
    "        df = woe_binning(df)\n",
    "        \n",
    "        df['iv'] = df['perc_diff'] * df['woe']\n",
    "        \n",
    "        return df, df['iv'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(train_x, train_y, run_num = 10, fold = 5):\n",
    "    \n",
    "    train_result, test_result = [], []\n",
    "    \n",
    "    for i in range(run_num):\n",
    "        \n",
    "        # result list\n",
    "        train_fold, test_fold = [], []\n",
    "        \n",
    "        # split dataset\n",
    "        skf = StratifiedKFold(n_splits = fold, shuffle = True)\n",
    "        \n",
    "        fold_num = 1\n",
    "        \n",
    "        for train_index, valid_index in skf.split(train_x, train_y):\n",
    "            \n",
    "            # dataset\n",
    "            X_train, X_valid = train_x.iloc[train_index], train_x.iloc[valid_index]\n",
    "            y_train, y_valid = train_y.iloc[train_index], train_y.iloc[valid_index]\n",
    "            \n",
    "            # model\n",
    "            reg = LogisticRegression(solver = \"liblinear\", penalty = \"l2\")\n",
    "            \n",
    "            reg.fit(X_train, y_train)\n",
    "            \n",
    "            y_train_pred = reg.predict(X_train)\n",
    "            \n",
    "            y_valid_pred = reg.predict(X_valid)\n",
    "            \n",
    "            # result AUC\n",
    "            train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "            test_auc = roc_auc_score(y_valid, y_valid_pred)\n",
    "            \n",
    "            if i == 1:\n",
    "                print(\"TRAIN Fold {0}, AUC score: {1}\".format(fold_num, round(train_auc, 4)))\n",
    "                print(\"TEST Fold {0}, AUC score: {1}\".format(fold_num, round(test_auc, 4)))\n",
    "            fold_num += 1\n",
    "            train_fold.append(train_auc)\n",
    "            test_fold.append(test_auc)\n",
    "        train_result.append(train_fold)\n",
    "        test_result.append(test_fold)\n",
    "    return train_result, test_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
